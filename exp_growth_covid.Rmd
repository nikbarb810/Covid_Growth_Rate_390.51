---
title: "Estimating rate of growth of SARS-CoV-2 in the European population"
output: html_document
date: "`r Sys.Date()`"
---

```{r}


#read observations
observed_file_path = "ms_obs_final.out"
obs_data = readLines(observed_file_path)




#read simulations
sim_file_path = "ms_sim_final.out"

# Read the file using readLines()
lines = readLines(sim_file_path)


# Specify the number of simulations
total_simulations = 10000

# Specify the number of rows per simulation
rows_per_simulation = 50

#final list containing all simulations
simulations = vector("list", total_simulations)

#formatting lines vector into simulations
for(curr_sim in 1:total_simulations) {
  start = curr_sim + (curr_sim - 1) * rows_per_simulation
  end = (curr_sim - 1) + curr_sim * rows_per_simulation
  
  simulations[[curr_sim]] = lines[start:end]
}

simulations

```



Our first parameter that we will calculate, will be the average number of pairwise differences between  all pairs of sequences. Below we have the implementation in R, but calling this function for all our simulations, severely slows the program down. 

The snippet after this one, shows a nice way to optimize the dreadfully slow for-loops that R uses, by "translating" the function, with the help of Rcpp, into cpp code that can be compiled and run significantly faster.

```{r}
library(stringdist)
library(Rcpp)



# calculates the pairwise difference between a pair of sequences.
pairwisediff <- function(arr1, arr2) {
  return (stringdist::stringdist(arr1, arr2, method = "hamming"))
}


# calculates the average number of pairwise difference between ALL pairs of sequences.
k_estimate <- function(simulation) {
  
  num_arrays = length(simulation)
  sum_diff = 0
  
  #sum all pairwise differences
  for(i in 1:(num_arrays - 1)) {
    for(j in (i + 1):num_arrays) {
      sum_diff = sum_diff + pairwisediff(simulation[[i]],simulation[[j]])
    }
  }
  
  
  #calculate average
  k_val = sum_diff / choose(num_arrays,2)
  
  return (k_val)
}
```


NOTE: Some packages might need to be installed in order to run Rcpp!

We can obtain the same functionality by the following cpp code, using Rcpp.

```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;

// Function to calculate pairwise differences
int pairwisediff_cpp(const std::string& str1, const std::string& str2) {
  int n = str1.size();
  int diff = 0;
  
  for (int i = 0; i < n; ++i) {
    if (str1[i] != str2[i]) {
      diff++;
    }
  }
  
  return diff;
}

// Function to calculate k estimate
// [[Rcpp::export]]
double k_estimate_cpp(const List& simulation) {
  int num_arrays = simulation.size();
  int sum_diff = 0;
  
  // Convert simulation indexes to C++ strings
  std::vector<std::string> strings(num_arrays);
  for (int i = 0; i < num_arrays; ++i) {
    strings[i] = Rcpp::as<std::string>(simulation[i]);
  }
  
  // Sum all pairwise differences
  for (int i = 0; i < (num_arrays - 1); ++i) {
    for (int j = (i + 1); j < num_arrays; ++j) {
      sum_diff += pairwisediff_cpp(strings[i], strings[j]);
    }
  }
  
  // Calculate average
  double k_val = static_cast<double>(sum_diff) / Rf_choose(num_arrays, 2);
  
  return k_val;
}

```



Below we can see a speed comparison between the 2 options, on a single random simulation that contains 2000 strings.
```{r}



# Set the length of the random vector
length <- 10

# Number of instances in the list
num_strings <- 2000


random_list <- replicate(num_strings, paste(sample(c("0", "1"), length, replace = TRUE), collapse = ""), simplify = FALSE)


# 96.38 seconds
exec_time <- system.time(k_estimate(random_list))
print(exec_time)

# 0.02 seconds
exec_time_cpp <- system.time(k_estimate_cpp(random_list))
print(exec_time_cpp)


```

We can definitely see that, implementing Rcpp code snippets, even in this small example has the potential to dramatically increase our performance.

But what's the difference in our actual database, which contains 10000 simulations of 50 strings each? 
```{r}

#calc time in r 
r_sims_time <- system.time(lapply(simulations,k_estimate))
print(r_sims_time) #606 seconds

#calc time in cpp
cpp_sims_time <- system.time(lapply(simulations,k_estimate_cpp))
print(cpp_sims_time) #1.1 seconds


#calculate actual values
k_vals_r = lapply(simulations,k_estimate) #wayyyy too slow
k_vals_cpp = lapply(simulations,k_estimate_cpp)


#compare 
comp <- mapply(identical,k_vals_r,k_vals_cpp)
k_vals_cpp[!comp] #list of not identical values is empty -> results match 

```




calculate w

```{r}


calc_a1 <- function(n) {
  a_1 = 0
  for(i in 1:(n-1)) {
    a_1 = a_1 + 1/i
  }
  
  return (a_1)
}

w_estimate <- function(s,n) {
  
  if(n == 0) {
    print("n must be positive!")
    return
  }

  a_1 = calc_a1(n)
  
  w = s / a_1
  
  return(w)
  
}

# ex = list(
#   c("0010100101"),
#   c("1001011101"),
#   c("1111111010")
# )
# print(ex)
# 
# 
# s_0 = nchar(ex[[1]])
# s_0
# n_0 = length(ex)
# 
# w = w_estimate(s_0,n_0)
# w



 

```

calculate Tajima's D

```{r}

calc_a2 <- function(n) {
  a_2 = 0
  for(i in 1:(n-1)) {
    a_2 = a_2 + (1/i)^2 
  }
  
  return (a_2)
}


D_estimation <- function(k,w,S,n) {
  
  if(n == 0 || n == 1) {
    print("ERROR: n must be greater than 1!")
    return
  }
  
  a_1 = calc_a1(n)
  a_2 = calc_a2(n)
  
  b_1 = (n + 1) / (3 * (n - 1))
  b_2 = (2 * (n^2 + n + 3)) / (9 * n * (n - 1))
  
  c_1 = b_1 - 1/a_1
  c_2 = b_2 - (n + 2) / (a_1 * n) + (a_2) / (a_1^2)
  
  e_1 = c_1/a_1
  e_2 = c_2/ (a_1^2 + a_2)
  
  
  d_num = k - w
  d_denom = (e_1 * S) + (e_2 * S) * (S - 1)
  
  D = d_num / sqrt(d_denom)
  
  return(D)
  
  
}



```

```{r}

tajimas_D <- function(simulation, show=0) {
  
  #calculate k
  k = k_estimate_cpp(simulation)
  
  
  #calculate w
  s = nchar(simulation[[1]])
  n = length(simulation)
  
  w = w_estimate(s,n)
  
  
  #calculate D
  d = D_estimation(k,w,s,n)
  
  
  if(show != 0) {
    print(paste("k:", k))
    print(paste("w:",w))
    print(paste("Tajima's D: ", d))
  }
    
  
  result <- c(k = k, w = w, d = d)
  return(result)
  
  
}

#calculate the parameters for all simulations
sims_params = sapply(simulations,tajimas_D)

#calculate the parameters for observed data
obs_params = tajimas_D(obs_data)

```


We now have calculated all 3 parameters for all of our simulations, as well as for the observed dataset. Before we try to approximate the parameter that calculated our observations, we have to normalize the simulations' and observation's data.

```{r}

#function takes as input a vector containing values of a parameter
#and returns a vector of the parameter's normalized values 
normalize_param <- function(param_vec) {
  
  params_mean = mean(param_vec)
  params_std = sqrt(var(param_vec))
  
  params_norm = (param_vec - params_mean) / params_std
  
  return(params_norm)
}

#function takes as input a single value for a parameter,
#as well as, the parameter's values in the simulation
#and returns the observed parameter normalized by the sims
normalize_obs_param <- function(obs_param, sim_param_vec) {
  
  params_mean = mean(sim_param_vec)
  params_std = sqrt(var(sim_param_vec))
  
  param_norm = (obs_param - params_mean) / params_std
  
  return (param_norm)
}

#normalize sims' parameters
norm_k = normalize_param(sims_params[1,])
norm_w = normalize_param(sims_params[2,])
norm_d = normalize_param(sims_params[3,])


sims_norm_params = lapply(1:total_simulations, function(i) list(k = norm_k[i], w = norm_w[i], d = norm_d[i]))

#normalize observed parameters
obs_norm_k = normalize_obs_param(obs_params[1],sims_params[1,])
obs_norm_w = normalize_obs_param(obs_params[2],sims_params[2,])
obs_norm_d = normalize_obs_param(obs_params[3],sims_params[3,])

#bundle results into a list
obs_norm_params = list(k = obs_norm_k, w = obs_norm_w, d = obs_norm_d)

```

At this point we have calculated and normalized all the values we need, to start approximating the parameter that was used in the observed database.

The approach that we will use is, find the 500 most similar simulations to our observed ones, and based on those, approximate the parameter that we are after.

To find these 500 simulations, we  will calculate the Euclidean distance between the observed and each of the simulated datasets.

```{r}

#function takes as input a list obs_params which has 3 indexes for each param
#as well as, another list that corresponds to the param of a simulation
#and returns their euclidian distance
eucl_dist <- function(obs_params, sim_params) {
  
  dist =  (obs_params$k - sim_params$k)^2 
        + (obs_params$w - sim_params$w)^2 
        + (obs_params$d - sim_params$d)^2
  
  return(sqrt(dist)[[1]])
  
}


#apply function to all simulations
dists = lapply(sims_norm_params, eucl_dist, obs_norm_params)
dists_vec = unlist(dists)

```

We have calculated each simulation's distance to our observed dataset. We need to keep in mind that, each simulation was calculated based on another parameter that we haven't talked about until now.

Our end goal is not to find the 500 simulations that are closest to our observations, but rather, the 500 parameters that were used to generate these simulations.

We must now, assosiate each distance with the parameter that generated it. The parameter list is in the file called pars_final.txt.


```{r}

#load pars_final file
params_file_path = "pars_final.txt"
generate_params = readLines(params_file_path) 


#associate parameters with their distance
params_dists = data.frame(dist = dists_vec, param = generate_params)
params_dists

#sort the dataframe based on the distances
sorted_indices = order(params_dists$dist)
sorted_params_dists = params_dists[sorted_indices, ]

#sorted dataframe
sorted_params_dists

#sorted params vector
sorted_params = as.numeric(sorted_params_dists$param)
sorted_params

#keep only the best results
threshold = 500
best_params = sorted_params[1:threshold]
best_params

```



